{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text preprocessing functions: \n",
    "# - clean_text: removes unwanted characters like numbers, punctuation, and special characters\n",
    "# - tokenize_and_remove_stopwords: splits text into tokens and removes stopwords\n",
    "# - lemmatize_text: reduces words to their base or root form\n",
    "from src.text_preprocessing import clean_text, tokenize_and_remove_stopwords, lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "df=pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \n",
       "0              Paperwhite voyage, no regrets!  \n",
       "1           One Simply Could Not Ask For More  \n",
       "2  Great for those that just want an e-reader  \n",
       "3                    Love / Hate relationship  \n",
       "4                                   I LOVE IT  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of dataset\n",
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text by removing numbers, punctuation, special characters, and extra spaces\n",
    "data['Cleaned_Review'] = data['reviews.text'].apply(clean_text)\n",
    "# Tokenize the cleaned text and remove stopwords\n",
    "data['Tokenized_NoStopwords_review'] = data['Cleaned_Review'].apply(tokenize_and_remove_stopwords)\n",
    "# Apply lemmatization to the tokens to get their base forms\n",
    "data['Lemmatized_Review'] = data['Tokenized_NoStopwords_review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for 'reviews.title' column\n",
    "data['Cleaned_title'] = data['reviews.title'].apply(clean_text)\n",
    "data['Tokenized_NoStopwords_title'] = data['Cleaned_title'].apply(tokenize_and_remove_stopwords)\n",
    "data['Lemmatized_title'] = data['Tokenized_NoStopwords_title'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized_NoStopwords_review</th>\n",
       "      <th>Lemmatized_Review</th>\n",
       "      <th>Cleaned_title</th>\n",
       "      <th>Tokenized_NoStopwords_title</th>\n",
       "      <th>Lemmatized_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>i initially had trouble deciding between the p...</td>\n",
       "      <td>[initially, trouble, deciding, paperwhite, voy...</td>\n",
       "      <td>[initially, trouble, decide, paperwhite, voyag...</td>\n",
       "      <td>paperwhite voyage no regrets</td>\n",
       "      <td>[paperwhite, voyage, regrets]</td>\n",
       "      <td>[paperwhite, voyage, regret]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>allow me to preface this with a little history...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>one simply could not ask for more</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>i am enjoying it so far great for reading had ...</td>\n",
       "      <td>[enjoying, far, great, reading, original, fire...</td>\n",
       "      <td>[enjoy, far, great, read, original, fire, sinc...</td>\n",
       "      <td>great for those that just want an ereader</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>i bought one of the first paperwhites and have...</td>\n",
       "      <td>[bought, one, first, paperwhites, pleased, con...</td>\n",
       "      <td>[buy, one, first, paperwhite, please, constant...</td>\n",
       "      <td>love hate relationship</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>i have to say upfront i dont like coroporate h...</td>\n",
       "      <td>[say, upfront, dont, like, coroporate, hermeti...</td>\n",
       "      <td>[say, upfront, do, not, like, coroporate, herm...</td>\n",
       "      <td>i love it</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \\\n",
       "0              Paperwhite voyage, no regrets!   \n",
       "1           One Simply Could Not Ask For More   \n",
       "2  Great for those that just want an e-reader   \n",
       "3                    Love / Hate relationship   \n",
       "4                                   I LOVE IT   \n",
       "\n",
       "                                      Cleaned_Review  \\\n",
       "0  i initially had trouble deciding between the p...   \n",
       "1  allow me to preface this with a little history...   \n",
       "2  i am enjoying it so far great for reading had ...   \n",
       "3  i bought one of the first paperwhites and have...   \n",
       "4  i have to say upfront i dont like coroporate h...   \n",
       "\n",
       "                        Tokenized_NoStopwords_review  \\\n",
       "0  [initially, trouble, deciding, paperwhite, voy...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoying, far, great, reading, original, fire...   \n",
       "3  [bought, one, first, paperwhites, pleased, con...   \n",
       "4  [say, upfront, dont, like, coroporate, hermeti...   \n",
       "\n",
       "                                   Lemmatized_Review  \\\n",
       "0  [initially, trouble, decide, paperwhite, voyag...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoy, far, great, read, original, fire, sinc...   \n",
       "3  [buy, one, first, paperwhite, please, constant...   \n",
       "4  [say, upfront, do, not, like, coroporate, herm...   \n",
       "\n",
       "                               Cleaned_title    Tokenized_NoStopwords_title  \\\n",
       "0               paperwhite voyage no regrets  [paperwhite, voyage, regrets]   \n",
       "1          one simply could not ask for more      [one, simply, could, ask]   \n",
       "2  great for those that just want an ereader         [great, want, ereader]   \n",
       "3                     love hate relationship     [love, hate, relationship]   \n",
       "4                                  i love it                         [love]   \n",
       "\n",
       "               Lemmatized_title  \n",
       "0  [paperwhite, voyage, regret]  \n",
       "1     [one, simply, could, ask]  \n",
       "2        [great, want, ereader]  \n",
       "3    [love, hate, relationship]  \n",
       "4                        [love]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use a regex to extract the 'amountMax' value from the 'prices' column\n",
    "# - The regex '\"amountMax\":\\s*(\\d+)' captures the numeric value after the 'amountMax' key\n",
    "# - Apply this extraction to each row in 'prices', handling null values with a conditional check\n",
    "data['price'] = data['prices'].apply(lambda x: re.search(r'\"amountMax\":\\s*(\\d+)', x).group(1) if pd.notnull(x) else None)\n",
    "\n",
    "# Convert the extracted 'price' values to float for numerical analysis\n",
    "data['price'] = data['price'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized_NoStopwords_review</th>\n",
       "      <th>Lemmatized_Review</th>\n",
       "      <th>Cleaned_title</th>\n",
       "      <th>Tokenized_NoStopwords_title</th>\n",
       "      <th>Lemmatized_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>i initially had trouble deciding between the p...</td>\n",
       "      <td>[initially, trouble, deciding, paperwhite, voy...</td>\n",
       "      <td>[initially, trouble, decide, paperwhite, voyag...</td>\n",
       "      <td>paperwhite voyage no regrets</td>\n",
       "      <td>[paperwhite, voyage, regrets]</td>\n",
       "      <td>[paperwhite, voyage, regret]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>allow me to preface this with a little history...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>one simply could not ask for more</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>i am enjoying it so far great for reading had ...</td>\n",
       "      <td>[enjoying, far, great, reading, original, fire...</td>\n",
       "      <td>[enjoy, far, great, read, original, fire, sinc...</td>\n",
       "      <td>great for those that just want an ereader</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>i bought one of the first paperwhites and have...</td>\n",
       "      <td>[bought, one, first, paperwhites, pleased, con...</td>\n",
       "      <td>[buy, one, first, paperwhite, please, constant...</td>\n",
       "      <td>love hate relationship</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>i have to say upfront i dont like coroporate h...</td>\n",
       "      <td>[say, upfront, dont, like, coroporate, hermeti...</td>\n",
       "      <td>[say, upfront, do, not, like, coroporate, herm...</td>\n",
       "      <td>i love it</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[love]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \\\n",
       "0              Paperwhite voyage, no regrets!   \n",
       "1           One Simply Could Not Ask For More   \n",
       "2  Great for those that just want an e-reader   \n",
       "3                    Love / Hate relationship   \n",
       "4                                   I LOVE IT   \n",
       "\n",
       "                                      Cleaned_Review  \\\n",
       "0  i initially had trouble deciding between the p...   \n",
       "1  allow me to preface this with a little history...   \n",
       "2  i am enjoying it so far great for reading had ...   \n",
       "3  i bought one of the first paperwhites and have...   \n",
       "4  i have to say upfront i dont like coroporate h...   \n",
       "\n",
       "                        Tokenized_NoStopwords_review  \\\n",
       "0  [initially, trouble, deciding, paperwhite, voy...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoying, far, great, reading, original, fire...   \n",
       "3  [bought, one, first, paperwhites, pleased, con...   \n",
       "4  [say, upfront, dont, like, coroporate, hermeti...   \n",
       "\n",
       "                                   Lemmatized_Review  \\\n",
       "0  [initially, trouble, decide, paperwhite, voyag...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoy, far, great, read, original, fire, sinc...   \n",
       "3  [buy, one, first, paperwhite, please, constant...   \n",
       "4  [say, upfront, do, not, like, coroporate, herm...   \n",
       "\n",
       "                               Cleaned_title    Tokenized_NoStopwords_title  \\\n",
       "0               paperwhite voyage no regrets  [paperwhite, voyage, regrets]   \n",
       "1          one simply could not ask for more      [one, simply, could, ask]   \n",
       "2  great for those that just want an ereader         [great, want, ereader]   \n",
       "3                     love hate relationship     [love, hate, relationship]   \n",
       "4                                  i love it                         [love]   \n",
       "\n",
       "               Lemmatized_title  price  \n",
       "0  [paperwhite, voyage, regret]  139.0  \n",
       "1     [one, simply, could, ask]  139.0  \n",
       "2        [great, want, ereader]  139.0  \n",
       "3    [love, hate, relationship]  139.0  \n",
       "4                        [love]  139.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1551 entries, 0 to 1550\n",
      "Data columns (total 17 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   id                            1551 non-null   object \n",
      " 1   asins                         1551 non-null   object \n",
      " 2   brand                         1551 non-null   object \n",
      " 3   categories                    1551 non-null   object \n",
      " 4   keys                          1551 non-null   object \n",
      " 5   name                          1551 non-null   object \n",
      " 6   prices                        1551 non-null   object \n",
      " 7   reviews.rating                1551 non-null   float64\n",
      " 8   reviews.text                  1551 non-null   object \n",
      " 9   reviews.title                 1551 non-null   object \n",
      " 10  Cleaned_Review                1551 non-null   object \n",
      " 11  Tokenized_NoStopwords_review  1551 non-null   object \n",
      " 12  Lemmatized_Review             1551 non-null   object \n",
      " 13  Cleaned_title                 1551 non-null   object \n",
      " 14  Tokenized_NoStopwords_title   1551 non-null   object \n",
      " 15  Lemmatized_title              1551 non-null   object \n",
      " 16  price                         1551 non-null   float64\n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 206.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking for dataframe informations\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed dataframe\n",
    "data.to_csv('../data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_preprocessing import clean_text, tokenize_and_remove_stopwords, lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.text_preprocessing' from 'c:\\\\Users\\\\AMINE AIT BELLAL\\\\Desktop\\\\amazon_reviews_analysis\\\\notebooks\\\\..\\\\src\\\\text_preprocessing.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from src import text_preprocessing\n",
    "reload(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_preprocessing import identify_rare_words,remove_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots rares (seuil=1) : ['pry', 'fingersfor', 'sundry', 'logistical', 'usability', 'kindleof', 'critique', 'ah', 'bookbub', 'alert', 'accumulative', 'positively', 'glacial', 'consensus', 'flakey', 'upfront', 'coroporate', 'hermetically', 'buti', 'itso', 'screenlight', 'disperse', 'outthe', 'paperwhites', 'guyi', 'funcion', 'quote', 'remains', 'lifeso', 'translate', 'simplify', 'moneytotally', 'loverill', 'mommy', 'feeding', 'cluster', 'growth', 'spurt', 'sidelyingbreastfeedingposition', 'disinfect', 'whenever', 'meanand', 'culture', 'usa', 'earlieri', 'modelpaper', 'canadian', 'belowthis', 'hurry', 'endsummary']\n",
      "Nombre total de mots rares : 2483\n"
     ]
    }
   ],
   "source": [
    "# Identifier les mots rares avec un seuil de 1\n",
    "rare_words = identify_rare_words(data['Lemmatized_Review'], threshold=1)\n",
    "\n",
    "# Afficher les mots rares\n",
    "print(f\"Mots rares (seuil=1) : {rare_words[:50]}\")  # Limité aux 50 premiers pour lisibilité\n",
    "print(f\"Nombre total de mots rares : {len(rare_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [initially, trouble, decide, paperwhite, voyag...\n",
      "1    [allow, preface, little, history, casual, read...\n",
      "2    [enjoy, far, great, read, original, fire, sinc...\n",
      "3    [buy, one, first, paperwhite, please, constant...\n",
      "4    [say, do, not, like, closed, stuff, like, anyt...\n",
      "Name: Filtered_ReviewTokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les mots rares des tokens\n",
    "data['Filtered_ReviewTokens'] = remove_rare_words(data['Lemmatized_Review'], rare_words)\n",
    "\n",
    "# Afficher un aperçu des tokens filtrés\n",
    "print(data['Filtered_ReviewTokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(text_preprocessing)\n",
    "from src.text_preprocessing import identify_misspelled_words, correct_spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots mal orthographiés (extrait) : ['yr', 'nifty', 'plaisir', 'ips', 'tab', 'wireless', 'luv', 'sooo', 'helpp', 'goto', 'builtin', 'oem', 'amost', 'hulu', 'wow', 'alexea', 'itwm', 'alt', 'comcast', 'hbo', 'kids', 'amazion', 'loos', 'refurb', 'epub', 'inexpensive', 'lineup', 'alexa', 'serviceim', 'trs', 'browse', 'tappy', 'grandma', 'echos', 'portability', 'quirk', 'dosent', 'bt', 'unusable', 'exceptionalan', 'audio', 'complment', 'redux', 'usb', 'warner', 'skip', 'apri', 'hdx', 'crap', 'importantan']\n",
      "Nombre total de mots mal orthographiés : 133\n"
     ]
    }
   ],
   "source": [
    "# Identifier les mots mal orthographiés\n",
    "misspelled_words = identify_misspelled_words(data['Lemmatized_title'])\n",
    "\n",
    "# Visualiser les premiers mots mal orthographiés\n",
    "print(f\"Mots mal orthographiés (extrait) : {list(misspelled_words)[:50]}\")\n",
    "print(f\"Nombre total de mots mal orthographiés : {len(misspelled_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Identifier les mots mal orthographiés\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m misspelled_words \u001b[38;5;241m=\u001b[39m \u001b[43midentify_misspelled_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFiltered_ReviewTokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Visualiser les premiers mots mal orthographiés\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMots mal orthographiés (extrait) : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(misspelled_words)[:\u001b[38;5;241m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AMINE AIT BELLAL\\Desktop\\amazon_reviews_analysis\\notebooks\\..\\src\\text_preprocessing.py:97\u001b[0m, in \u001b[0;36midentify_misspelled_words\u001b[1;34m(tokens_column)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     96\u001b[0m             blob \u001b[38;5;241m=\u001b[39m TextBlob(word)\n\u001b[1;32m---> 97\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m word:  \u001b[38;5;66;03m# Vérifie si le mot corrigé est différent\u001b[39;00m\n\u001b[0;32m     98\u001b[0m                 misspelled_words\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m misspelled_words\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\blob.py:549\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    547\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    548\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m--> 549\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\blob.py:548\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[0;32m    547\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 548\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m    549\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\blob.py:115\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\blob.py:107\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\en\\__init__.py:118\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of (word, confidence)-tuples of spelling corrections.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\_text.py:1692\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)]  \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w])\n\u001b[0;32m   1691\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w))\n\u001b[1;32m-> 1692\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[0;32m   1694\u001b[0m )\n\u001b[0;32m   1695\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[0;32m   1696\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\_text.py:1667\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\_text.py:1667\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\_text.py:1659\u001b[0m, in \u001b[0;36mSpelling._edit1\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m   1655\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1656\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m   1657\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1658\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m-> 1659\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA],\n\u001b[0;32m   1660\u001b[0m )\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\site-packages\\textblob\\_text.py:1659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m   1655\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1656\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m   1657\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1658\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m-> 1659\u001b[0m     [\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA],\n\u001b[0;32m   1660\u001b[0m )\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Identifier les mots mal orthographiés\n",
    "misspelled_words = identify_misspelled_words(data['Filtered_ReviewTokens'])\n",
    "\n",
    "# Visualiser les premiers mots mal orthographiés\n",
    "print(f\"Mots mal orthographiés (extrait) : {list(misspelled_words)[:50]}\")\n",
    "print(f\"Nombre total de mots mal orthographiés : {len(misspelled_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Filtered_ReviewTokens  \\\n",
      "0  [initially, trouble, decide, paperwhite, voyag...   \n",
      "1  [allow, preface, little, history, casual, read...   \n",
      "2  [enjoy, far, great, read, original, fire, sinc...   \n",
      "3  [buy, one, first, paperwhite, please, constant...   \n",
      "4  [say, do, not, like, closed, stuff, like, anyt...   \n",
      "\n",
      "                            Corrected_filtred_Tokens  \n",
      "0  [initially, trouble, decide, paperwhite, voyag...  \n",
      "1  [allow, preface, little, history, casual, read...  \n",
      "2  [enjoy, far, great, read, original, fire, sinc...  \n",
      "3  [buy, one, first, paperwhite, please, constant...  \n",
      "4  [say, do, not, like, closed, stuff, like, anyt...  \n"
     ]
    }
   ],
   "source": [
    "# Appliquer la correction d'orthographe\n",
    "data['Corrected_filtred_Tokens'] = correct_spelling(data['Filtered_ReviewTokens'])\n",
    "\n",
    "# Visualiser un aperçu des tokens corrigés\n",
    "print(data[['Filtered_ReviewTokens', 'Corrected_filtred_Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed dataframe\n",
    "data.to_csv('../data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(text_preprocessing)\n",
    "from src.text_preprocessing import call_gensim_bigram_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AMINE AIT BELLAL\\Desktop\\amazon_reviews_analysis\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['C:/Miniconda3/envs/gensim_env\\\\Scripts\\\\python.exe', 'src/gensim_functions.py', 'tokens_column.pkl', 'enriched_tokens.pkl']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Appliquer la fonction aux tokens corrigés\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnriched_Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gensim_bigram_trigram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCorrected_filtred_Tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Aperçu des résultats\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected_filtred_Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnriched_Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\AMINE AIT BELLAL\\Desktop\\amazon_reviews_analysis\\notebooks\\..\\src\\text_preprocessing.py:144\u001b[0m, in \u001b[0;36mcall_gensim_bigram_trigram\u001b[1;34m(tokens_column)\u001b[0m\n\u001b[0;32m    141\u001b[0m command \u001b[38;5;241m=\u001b[39m [gensim_python, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/gensim_functions.py\u001b[39m\u001b[38;5;124m'\u001b[39m, input_file, output_file]\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Exécuter la commande\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Charger les résultats depuis le fichier de sortie\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\amazon_venv\\lib\\subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['C:/Miniconda3/envs/gensim_env\\\\Scripts\\\\python.exe', 'src/gensim_functions.py', 'tokens_column.pkl', 'enriched_tokens.pkl']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction aux tokens corrigés\n",
    "data['Enriched_Tokens'] = call_gensim_bigram_trigram(data['Corrected_filtred_Tokens'])\n",
    "\n",
    "# Aperçu des résultats\n",
    "print(data[['Corrected_filtred_Tokens', 'Enriched_Tokens']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
