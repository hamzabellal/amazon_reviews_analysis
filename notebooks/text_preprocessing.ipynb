{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text preprocessing functions: \n",
    "# - clean_text: removes unwanted characters like numbers, punctuation, and special characters\n",
    "# - tokenize_and_remove_stopwords: splits text into tokens and removes stopwords\n",
    "# - lemmatize_text: reduces words to their base or root form\n",
    "from src.text_preprocessing import clean_text, tokenize_and_remove_stopwords, lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "df=pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \n",
       "0              Paperwhite voyage, no regrets!  \n",
       "1           One Simply Could Not Ask For More  \n",
       "2  Great for those that just want an e-reader  \n",
       "3                    Love / Hate relationship  \n",
       "4                                   I LOVE IT  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of dataset\n",
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text by removing numbers, punctuation, special characters, and extra spaces\n",
    "data['Cleaned_Review'] = data['reviews.text'].apply(clean_text)\n",
    "# Tokenize the cleaned text and remove stopwords\n",
    "data['Tokenized_NoStopwords_review'] = data['Cleaned_Review'].apply(tokenize_and_remove_stopwords)\n",
    "# Apply lemmatization to the tokens to get their base forms\n",
    "data['Lemmatized_Review'] = data['Tokenized_NoStopwords_review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for 'reviews.title' column\n",
    "data['Cleaned_title'] = data['reviews.title'].apply(clean_text)\n",
    "data['Tokenized_NoStopwords_title'] = data['Cleaned_title'].apply(tokenize_and_remove_stopwords)\n",
    "data['Lemmatized_title'] = data['Tokenized_NoStopwords_title'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized_NoStopwords_review</th>\n",
       "      <th>Lemmatized_Review</th>\n",
       "      <th>Cleaned_title</th>\n",
       "      <th>Tokenized_NoStopwords_title</th>\n",
       "      <th>Lemmatized_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>i initially had trouble deciding between the p...</td>\n",
       "      <td>[initially, trouble, deciding, paperwhite, voy...</td>\n",
       "      <td>[initially, trouble, decide, paperwhite, voyag...</td>\n",
       "      <td>paperwhite voyage no regrets</td>\n",
       "      <td>[paperwhite, voyage, regrets]</td>\n",
       "      <td>[paperwhite, voyage, regret]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>allow me to preface this with a little history...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>one simply could not ask for more</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>i am enjoying it so far great for reading had ...</td>\n",
       "      <td>[enjoying, far, great, reading, original, fire...</td>\n",
       "      <td>[enjoy, far, great, read, original, fire, sinc...</td>\n",
       "      <td>great for those that just want an ereader</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>i bought one of the first paperwhites and have...</td>\n",
       "      <td>[bought, one, first, paperwhites, pleased, con...</td>\n",
       "      <td>[buy, one, first, paperwhite, please, constant...</td>\n",
       "      <td>love hate relationship</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>i have to say upfront i dont like coroporate h...</td>\n",
       "      <td>[say, upfront, dont, like, coroporate, hermeti...</td>\n",
       "      <td>[say, upfront, do, not, like, coroporate, herm...</td>\n",
       "      <td>i love it</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \\\n",
       "0              Paperwhite voyage, no regrets!   \n",
       "1           One Simply Could Not Ask For More   \n",
       "2  Great for those that just want an e-reader   \n",
       "3                    Love / Hate relationship   \n",
       "4                                   I LOVE IT   \n",
       "\n",
       "                                      Cleaned_Review  \\\n",
       "0  i initially had trouble deciding between the p...   \n",
       "1  allow me to preface this with a little history...   \n",
       "2  i am enjoying it so far great for reading had ...   \n",
       "3  i bought one of the first paperwhites and have...   \n",
       "4  i have to say upfront i dont like coroporate h...   \n",
       "\n",
       "                        Tokenized_NoStopwords_review  \\\n",
       "0  [initially, trouble, deciding, paperwhite, voy...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoying, far, great, reading, original, fire...   \n",
       "3  [bought, one, first, paperwhites, pleased, con...   \n",
       "4  [say, upfront, dont, like, coroporate, hermeti...   \n",
       "\n",
       "                                   Lemmatized_Review  \\\n",
       "0  [initially, trouble, decide, paperwhite, voyag...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoy, far, great, read, original, fire, sinc...   \n",
       "3  [buy, one, first, paperwhite, please, constant...   \n",
       "4  [say, upfront, do, not, like, coroporate, herm...   \n",
       "\n",
       "                               Cleaned_title    Tokenized_NoStopwords_title  \\\n",
       "0               paperwhite voyage no regrets  [paperwhite, voyage, regrets]   \n",
       "1          one simply could not ask for more      [one, simply, could, ask]   \n",
       "2  great for those that just want an ereader         [great, want, ereader]   \n",
       "3                     love hate relationship     [love, hate, relationship]   \n",
       "4                                  i love it                         [love]   \n",
       "\n",
       "               Lemmatized_title  \n",
       "0  [paperwhite, voyage, regret]  \n",
       "1     [one, simply, could, ask]  \n",
       "2        [great, want, ereader]  \n",
       "3    [love, hate, relationship]  \n",
       "4                        [love]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use a regex to extract the 'amountMax' value from the 'prices' column\n",
    "# - The regex '\"amountMax\":\\s*(\\d+)' captures the numeric value after the 'amountMax' key\n",
    "# - Apply this extraction to each row in 'prices', handling null values with a conditional check\n",
    "data['price'] = data['prices'].apply(lambda x: re.search(r'\"amountMax\":\\s*(\\d+)', x).group(1) if pd.notnull(x) else None)\n",
    "\n",
    "# Convert the extracted 'price' values to float for numerical analysis\n",
    "data['price'] = data['price'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>name</th>\n",
       "      <th>prices</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized_NoStopwords_review</th>\n",
       "      <th>Lemmatized_Review</th>\n",
       "      <th>Cleaned_title</th>\n",
       "      <th>Tokenized_NoStopwords_title</th>\n",
       "      <th>Lemmatized_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>i initially had trouble deciding between the p...</td>\n",
       "      <td>[initially, trouble, deciding, paperwhite, voy...</td>\n",
       "      <td>[initially, trouble, decide, paperwhite, voyag...</td>\n",
       "      <td>paperwhite voyage no regrets</td>\n",
       "      <td>[paperwhite, voyage, regrets]</td>\n",
       "      <td>[paperwhite, voyage, regret]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>allow me to preface this with a little history...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>[allow, preface, little, history, casual, read...</td>\n",
       "      <td>one simply could not ask for more</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>[one, simply, could, ask]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>i am enjoying it so far great for reading had ...</td>\n",
       "      <td>[enjoying, far, great, reading, original, fire...</td>\n",
       "      <td>[enjoy, far, great, read, original, fire, sinc...</td>\n",
       "      <td>great for those that just want an ereader</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>[great, want, ereader]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>i bought one of the first paperwhites and have...</td>\n",
       "      <td>[bought, one, first, paperwhites, pleased, con...</td>\n",
       "      <td>[buy, one, first, paperwhite, please, constant...</td>\n",
       "      <td>love hate relationship</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>[love, hate, relationship]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>[{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>i have to say upfront i dont like coroporate h...</td>\n",
       "      <td>[say, upfront, dont, like, coroporate, hermeti...</td>\n",
       "      <td>[say, upfront, do, not, like, coroporate, herm...</td>\n",
       "      <td>i love it</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[love]</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "                          keys               name  \\\n",
       "0  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "1  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "2  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "3  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "4  kindlepaperwhite/b00qjdu3ky  Kindle Paperwhite   \n",
       "\n",
       "                                              prices  reviews.rating  \\\n",
       "0  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "1  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "2  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             4.0   \n",
       "3  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "4  [{\"amountMax\":139.99,\"amountMin\":139.99,\"curre...             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title  \\\n",
       "0              Paperwhite voyage, no regrets!   \n",
       "1           One Simply Could Not Ask For More   \n",
       "2  Great for those that just want an e-reader   \n",
       "3                    Love / Hate relationship   \n",
       "4                                   I LOVE IT   \n",
       "\n",
       "                                      Cleaned_Review  \\\n",
       "0  i initially had trouble deciding between the p...   \n",
       "1  allow me to preface this with a little history...   \n",
       "2  i am enjoying it so far great for reading had ...   \n",
       "3  i bought one of the first paperwhites and have...   \n",
       "4  i have to say upfront i dont like coroporate h...   \n",
       "\n",
       "                        Tokenized_NoStopwords_review  \\\n",
       "0  [initially, trouble, deciding, paperwhite, voy...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoying, far, great, reading, original, fire...   \n",
       "3  [bought, one, first, paperwhites, pleased, con...   \n",
       "4  [say, upfront, dont, like, coroporate, hermeti...   \n",
       "\n",
       "                                   Lemmatized_Review  \\\n",
       "0  [initially, trouble, decide, paperwhite, voyag...   \n",
       "1  [allow, preface, little, history, casual, read...   \n",
       "2  [enjoy, far, great, read, original, fire, sinc...   \n",
       "3  [buy, one, first, paperwhite, please, constant...   \n",
       "4  [say, upfront, do, not, like, coroporate, herm...   \n",
       "\n",
       "                               Cleaned_title    Tokenized_NoStopwords_title  \\\n",
       "0               paperwhite voyage no regrets  [paperwhite, voyage, regrets]   \n",
       "1          one simply could not ask for more      [one, simply, could, ask]   \n",
       "2  great for those that just want an ereader         [great, want, ereader]   \n",
       "3                     love hate relationship     [love, hate, relationship]   \n",
       "4                                  i love it                         [love]   \n",
       "\n",
       "               Lemmatized_title  price  \n",
       "0  [paperwhite, voyage, regret]  139.0  \n",
       "1     [one, simply, could, ask]  139.0  \n",
       "2        [great, want, ereader]  139.0  \n",
       "3    [love, hate, relationship]  139.0  \n",
       "4                        [love]  139.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1551 entries, 0 to 1550\n",
      "Data columns (total 17 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   id                            1551 non-null   object \n",
      " 1   asins                         1551 non-null   object \n",
      " 2   brand                         1551 non-null   object \n",
      " 3   categories                    1551 non-null   object \n",
      " 4   keys                          1551 non-null   object \n",
      " 5   name                          1551 non-null   object \n",
      " 6   prices                        1551 non-null   object \n",
      " 7   reviews.rating                1551 non-null   float64\n",
      " 8   reviews.text                  1551 non-null   object \n",
      " 9   reviews.title                 1551 non-null   object \n",
      " 10  Cleaned_Review                1551 non-null   object \n",
      " 11  Tokenized_NoStopwords_review  1551 non-null   object \n",
      " 12  Lemmatized_Review             1551 non-null   object \n",
      " 13  Cleaned_title                 1551 non-null   object \n",
      " 14  Tokenized_NoStopwords_title   1551 non-null   object \n",
      " 15  Lemmatized_title              1551 non-null   object \n",
      " 16  price                         1551 non-null   float64\n",
      "dtypes: float64(2), object(15)\n",
      "memory usage: 206.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking for dataframe informations\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated preprocessed dataframe\n",
    "data.to_csv('../data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, several text preprocessing steps were performed to clean and prepare the data for analysis. The text was converted to lowercase, with URLs, mentions, punctuation, and numbers removed, along with unnecessary whitespace, to standardize the text. Sentences were tokenized into individual words, and stopwords were removed to reduce noise. Lemmatization was applied to normalize words to their canonical forms (e.g., \"running\" to \"run\"), ensuring consistency across the dataset. Additionally, relevant numerical information, such as maximum prices, was extracted using regular expressions to enhance the dataset with meaningful features for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from src import text_preprocessing\n",
    "R=reload(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the functions `identify_rare_words` and `remove_rare_words` from the `text_preprocessing` module.\n",
    "# - `identify_rare_words`: This function identifies words that appear infrequently in the dataset, \n",
    "#   which can be considered as rare words based on a defined threshold.\n",
    "# - `remove_rare_words`: This function removes the identified rare words from the dataset, \n",
    "#   reducing noise and improving the focus on more meaningful and commonly occurring words.\n",
    "\n",
    "from src.text_preprocessing import identify_rare_words,remove_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rare words (threshold=1) : ['pry', 'fingersfor', 'sundry', 'logistical', 'usability', 'kindleof', 'critique', 'ah', 'bookbub', 'alert', 'accumulative', 'positively', 'glacial', 'consensus', 'flakey', 'upfront', 'coroporate', 'hermetically', 'buti', 'itso', 'screenlight', 'disperse', 'outthe', 'paperwhites', 'guyi', 'funcion', 'quote', 'remains', 'lifeso', 'translate', 'simplify', 'moneytotally', 'loverill', 'mommy', 'feeding', 'cluster', 'growth', 'spurt', 'sidelyingbreastfeedingposition', 'disinfect', 'whenever', 'meanand', 'culture', 'usa', 'earlieri', 'modelpaper', 'canadian', 'belowthis', 'hurry', 'endsummary']\n",
      "count of rare words : 2483\n"
     ]
    }
   ],
   "source": [
    "# using identify_rare_words function to extract words with freq=1\n",
    "rare_words = identify_rare_words(data['Lemmatized_Review'], threshold=1)\n",
    "\n",
    "# Output\n",
    "print(f\"rare words (threshold=1) : {rare_words[:50]}\")  # Limited at 50 first rows for lisiblity\n",
    "print(f\"count of rare words : {len(rare_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [initially, trouble, decide, paperwhite, voyag...\n",
      "1    [allow, preface, little, history, casual, read...\n",
      "2    [enjoy, far, great, read, original, fire, sinc...\n",
      "3    [buy, one, first, paperwhite, please, constant...\n",
      "4    [say, do, not, like, closed, stuff, like, anyt...\n",
      "Name: Filtered_ReviewTokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Delete rare words with freq=1 from tokens lists\n",
    "data['Filtered_ReviewTokens'] = remove_rare_words(data['Lemmatized_Review'], rare_words)\n",
    "\n",
    "# print the output\n",
    "print(data['Filtered_ReviewTokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `identify_misspelled_words`: This function detects words in the dataset that are likely misspelled, \n",
    "# helping to identify inconsistencies in the text data.\n",
    "# `correct_spelling`: This function corrects the identified misspelled words, \n",
    "# ensuring consistency and improving the quality of the text data for analysis.\n",
    "\n",
    "reload(text_preprocessing)\n",
    "from src.text_preprocessing import identify_misspelled_words, correct_spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of misspelled words : ['yr', 'nifty', 'plaisir', 'ips', 'tab', 'wireless', 'luv', 'sooo', 'helpp', 'goto', 'builtin', 'oem', 'amost', 'hulu', 'wow', 'alexea', 'itwm', 'alt', 'comcast', 'hbo', 'kids', 'amazion', 'loos', 'refurb', 'epub', 'inexpensive', 'lineup', 'alexa', 'serviceim', 'trs', 'browse', 'tappy', 'grandma', 'echos', 'portability', 'quirk', 'dosent', 'bt', 'unusable', 'exceptionalan', 'audio', 'complment', 'redux', 'usb', 'warner', 'skip', 'apri', 'hdx', 'crap', 'importantan']\n",
      "Total number of misspelled words : 133\n"
     ]
    }
   ],
   "source": [
    "# Identify misspelled words\n",
    "misspelled_words = identify_misspelled_words(data['Lemmatized_title'])\n",
    "\n",
    "# Visualize the first few misspelled words\n",
    "print(f\"Sample of misspelled words : {list(misspelled_words)[:50]}\")\n",
    "print(f\"Total number of misspelled words : {len(misspelled_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Identify misspelled words\\nmisspelled_words = identify_misspelled_words(data[\\'Filtered_ReviewTokens\\'])\\n\\n# Display the first few misspelled words\\nprint(f\"Misspelled words (sample): {list(misspelled_words)[:50]}\")\\nprint(f\"Total number of misspelled words: {len(misspelled_words)}\")'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Identify misspelled words\n",
    "misspelled_words = identify_misspelled_words(data['Filtered_ReviewTokens'])\n",
    "\n",
    "# Display the first few misspelled words\n",
    "print(f\"Misspelled words (sample): {list(misspelled_words)[:50]}\")\n",
    "print(f\"Total number of misspelled words: {len(misspelled_words)}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Filtered_ReviewTokens  \\\n",
      "0  [initially, trouble, decide, paperwhite, voyag...   \n",
      "1  [allow, preface, little, history, casual, read...   \n",
      "2  [enjoy, far, great, read, original, fire, sinc...   \n",
      "3  [buy, one, first, paperwhite, please, constant...   \n",
      "4  [say, do, not, like, closed, stuff, like, anyt...   \n",
      "\n",
      "                            Corrected_filtred_Tokens  \n",
      "0  [initially, trouble, decide, paperwhite, voyag...  \n",
      "1  [allow, preface, little, history, casual, read...  \n",
      "2  [enjoy, far, great, read, original, fire, sinc...  \n",
      "3  [buy, one, first, paperwhite, please, constant...  \n",
      "4  [say, do, not, like, closed, stuff, like, anyt...  \n"
     ]
    }
   ],
   "source": [
    "# Apply spelling correction\n",
    "data['Corrected_filtred_Tokens'] = correct_spelling(data['Filtered_ReviewTokens'])\n",
    "\n",
    "# Display a preview of the corrected tokens\n",
    "print(data[['Filtered_ReviewTokens', 'Corrected_filtred_Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated preprocessed dataframe\n",
    "data.to_csv('../data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(text_preprocessing)\n",
    "from src.text_preprocessing import call_gensim_bigram_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ../src/gensim_functions.py exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(script_path):\n",
    "    print(f\"file {script_path} exist.\")\n",
    "else:\n",
    "    print(f\"file {script_path} does not exist. Verify path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Corrected_filtred_Tokens  \\\n",
      "0  [initially, trouble, decide, paperwhite, voyag...   \n",
      "1  [allow, preface, little, history, casual, read...   \n",
      "2  [enjoy, far, great, read, original, fire, sinc...   \n",
      "3  [buy, one, first, paperwhite, please, constant...   \n",
      "4  [say, do, not, like, closed, stuff, like, anyt...   \n",
      "\n",
      "                                     Enriched_Tokens  \n",
      "0  [initially, trouble, decide, paperwhite_voyage...  \n",
      "1  [allow, preface, little, history, casual, read...  \n",
      "2  [enjoy, far, great, read, original, fire, sinc...  \n",
      "3  [buy, one, first, paperwhite, please, constant...  \n",
      "4  [say_do_not, like, closed, stuff, like, anythi...  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Path to the parent directory of the src folder.\n",
    "script_path='../src/gensim_functions.py'\n",
    "# Apply the function on corrected tokens\n",
    "data['Enriched_Tokens'] = call_gensim_bigram_trigram(data['Corrected_filtred_Tokens'],script_path)\n",
    "\n",
    "# results\n",
    "print(data[['Corrected_filtred_Tokens', 'Enriched_Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the advanced preprocessing stage, additional steps were performed to enhance the quality of the text data. Words with a frequency of only one were filtered out, as they provide little to no value for analysis. Misspelled words were then identified and corrected using an integrated dictionary to ensure consistency and accuracy. Following this, bigrams and trigrams were generated to capture meaningful multi-word phrases, improving the representation of context and relationships within the text. These steps further refined the dataset, making it more suitable for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated preprocessed dataframe\n",
    "data.to_csv('../data/preprocessed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
